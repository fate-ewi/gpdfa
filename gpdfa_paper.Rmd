---
title: Smoothed factor analysis for identifying trends in multivariate time series
author: Eric J. Ward$^1$, Mike A. Litzow$^2$, Mary E. Hunsicker$^3$, Sean C. Anderson$^4$
output:
  pdf_document:
    fig_caption: yes
    includes:
      in_header: options.sty
    latex_engine: xelatex
  word_document: default
  html_document: default
  bookdown::pdf_document2:
csl: ecology.csl
bibliography: gpdfa.bib
---

<!-- Submissions may contain up to 20 manuscript pages. Statistical Reports mansucripts must conform to a strict page limit and format. The manuscript text (including the Abstract), literature cited, figure captions, and table captions/notes/footnotes must be double-spaced in 12-point Times New Roman font. The abstract can have a maximum of 200 words. -->

<!-- The parts of the manuscript should be assembled in this order: title page, abstract (on a new page), key words, text, acknowledgments, literature cited, tables (starting on a new page, one table per page, the caption should be included with the table), figure captions (on separate page preceding the first figure; captions should follow each other continuously and not on separate pages), figures (one figure per page; label each figure, i.e., Figure 1, Figure 2, etc.), and lastly any Appendices/Supporting Information (with each Appendix presented in separate files). -->
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy=FALSE, tidy.opts=list(width.cutoff=60), warning = FALSE, message = FALSE)
library(bayesdfa)
library(knitr)
library(tidyverse)
library(ggsidekick)
library(ggrepel)
library(viridis)
library(gridExtra)
library(rstan)
library(ggrepel)
library(cowplot)
```

$^1$Conservation Biology Division, Northwest Fisheries Science Center, National Marine Fisheries Service, National Oceanic and Atmospheric Administration, 2725 Montlake Blvd E, Seattle WA, 98112, USA\
$^2$Shellfish Assessment Program, Alaska Fisheries Science Center, National Marine Fisheries Service, National Oceanic and Atmospheric Administration, 301 Research Court. Kodiak , AK, 99615, USA\
$^3$Fish Ecology Division, Northwest Fisheries Science Center, National Marine Fisheries Service, National Oceanic and Atmospheric Administration, 2725 Montlake Blvd E, Seattle WA, 98112, USA\
$^4$Pacific Biological Station, Fisheries and Oceans Canada, Nanaimo, BC, V6T 6N7, Canada 

\break 

## Abstract
\setlength\parindent{24pt}

Ecological processes are rarely directly observable, and for most systems, variance parameters must be estimated from data. State space models have become widely used in the environmental sciences, particularly for time series data, because of their ability to simultaneously estimate multiple sources of variation (process or natural variability, and variance attributed to observations, associated with measurement and sampling errors). A common state space approach for using multivariate time series to identify underlying signals is dynamic factor analysis (DFA). The conventional DFA model is flexible in that unseen processes are modeled as random walks. Whereas this may be suitable for some situations, random walks may be too flexible for other cases. In this paper, we introduce a new class of DFA models, where latent processes are modeled as smooth functions. We highlight two alternatives to the random walk approach, using basis splines and Gaussian predictive process models. These models are applied to two long-term datasets from the west coast of the United States: (1) a 35-year dataset of juvenile rockfishes from the west coast of the United States, and (2) a 39-year dataset of fisheries catches. We compare the out-of-sample predictive accuracy of our new approaches to conventional models with cross-validation, and find that for both applications, the smooth trend models result in models with higher predictive accuracy.

## Key words
Dynamic factor analysis, smooth spline, B-spline, Gaussian process, Bayesian modeling, Stan

\break 

## Introduction

Ecological data can be characterized by multiple sources of variability, including stochastic natural variation, and errors associated with data collection (observation, sampling, and measurement errors). Disentangling these sources of variability is often challenging, and necessitates the use of statistical methods, such as state space models. These approaches have become ubiquitous in ecology, particularly for time-series data [@auger-methe2020]---in part because these models allow researchers to make inferences about ecological processes that are not directly observable. Applications of these models include estimating population change over time [@clark2004], movement dynamics [@patterson2008], and understanding spatiotemporal variation [@anderson2019b].

Estimating multiple sources of variation in state space models is numerically complex, and can be constrained explicitly or implicitly in ecological models via model assumptions. For example, discrete-time state-space models of population trajectories generally assume latent population size $n_t$ at time $t$ can be approximated by an autoregressive process in log-space, $x_{t+1}=f(x_{t})+\epsilon_t$, where $f()$ represents some function, $x_{t}=\log(n_{t})$, and $\epsilon_t$ are normally distributed process deviations representing stochastic variability of the natural system [@dennis2006]. The autoregressive assumption is critical here; without such a constraint, the variance of the stochastic noise $\epsilon_t$ is not estimable in the presence of an observation or data model. Separating these sources of variability is critical to generating unbiased estimates of population trends or density dependence [@knape2008a]. If inference is not dependent on parameters of ecological interest (e.g., growth rates, density dependence), a wide range of alternative semi-parametric approaches exist that can be used to model the trajectory of $x_t$, including generalized additive models [GAMs, @wood2011] and Gaussian process models [@roberts2013]. Because these models are not autoregressive with discrete time steps, the flexibility or 'wiggliness' of the model can be adjusted as part of the model fitting. In addition to their flexibility, these semi-parametric models may be better suited for situations when data are patchily distributed in time or unequally spaced, making estimation of process and observation errors more difficult.

Challenges posed by univariate time-series models also apply to multivariate time-series models, with the additional complexity that the number of latent time series may be variable, $k=1, \ldots ,m$, where $m$ is the number of time series observed. At one extreme, $k=m$, and each time series corresponds to a unique latent state. Motivating questions in analyzing these kinds of data include estimating correlated latent processes or trends, or estimating effects of environmental covariates on sub-populations [@hovel2016]. At the other extreme, $k=1$, where each time series represents replicates or multiple measurements of the same trajectory of states, with optional offsets or coefficients included for each time series (e.g., offsets allowing for differing detectability). Applications focused on estimating a single trend from multivariate data include the development of ecological indicators. Models with intermediate numbers of latent states $1<k<m$ require mapping of time series to latent trends. These may be specified a priori [@ward2010c] or estimated within the modeling framework using dimension reduction techniques. 

Many statistical approaches have been proposed in recent years for clustering or estimating common signals in multivariate time series [@liao2005]. Examples include clustering based on similarities among time series features [@sarda-espinosa2019], identifying common patterns in the frequency domain [@holan2018], and clustering based on neural networks [@cherif2011]. Application of these methods to ecological data has been limited, however, in part because many of these approaches identify clusters from raw data and ignore observation error. An alternative approach that has been used in ecology to map collections of multivariate time series to latent states, while accounting for observation error, is dynamic factor analysis (DFA) [@zuur2003b; @zuur2003c]. DFA is an extension of factor analysis for time series data, and estimates a small number of unobserved processes, referred to as trends, that can describe observed data. Mapping time series to trends is done via estimated factor loadings---these allow each time series to be modeled as a mixture of the estimated latent trends, rather than assigning each time series to a single trend.

To date, applications of DFA models in ecology and other fields have assumed that underlying trends are modeled as a random walk, $x_{t+1}=x_{t}+\epsilon_t$. The objective of this analysis is to introduce a new class of DFA models for multivariate time series. Just as the univariate autoregressive model described above can be approximated with smooth functions, DFA models may be extended to use smooth functions in lieu of autoregressive processes. Recent work has highlighted the application of hierarchical GAMs for multiple data sources [@pedersen2019]. These approaches are flexible and likely to provide similar inference to DFA for single latent trend; however, these methods have not been extended to include more than one process. We illustrate two options for modeling smooth functions for latent trends: basis splines ('B-splines') and Gaussian process models. We compare both approaches to conventional autoregressive DFA models for two datasets on marine fishes from the west coast of the USA. All data and code for replicating our analysis are available on Github (*link here*) and Zenodo (*link here*), and in our existing R package 'bayesdfa' [@ward2019].

## Methods

### Dynamic Factor Model  

The basic DFA model can be written as a multivariate state space model, consisting of a latent process model and observation or data model. In its simplest form, the process model is expressed as a random walk, $\textbf{x}_{t+1}=\textbf{x}_{t}+\textbf{w}_{t}$, where $\textbf{w}_{t}\sim \mathrm{MVN}(\textbf{0},\textbf{Q})$. For identifiability constraints, the covariance matrix $\textbf{Q}$ is generally constrained to be an identity matrix [@zuur2003b; @holmes2012b]. Additional features may be incorporated into the process model, including autoregressive or moving average coefficients, covariates, or deviations that are more extreme than that of the normal distribution [@ward2019]. The observation model in a DFA is expressed as a linear combination of trends $\textbf{x}_{t}$ and a matrix of loadings coefficients $\textbf{Z}$, $\textbf{y}_{t} = \textbf{Z}\textbf{x}_{t}+\textbf{B}\textbf{d}_{t}+\textbf{e}_{t}$. In addition to the trends and loadings, time-varying covariates $\textbf{d}_t$ may be optionally included and linked to the observations through estimated coefficients $\textbf{B}$. The vector $\textbf{e}_t$ represents residual observation error, which is typically modeled as a diagonal matrix, $\textbf{e}_{t}\sim \mathrm{MVN}(0,\textbf{R})$, although off-diagonal elements may be estimated [@holmes2020]. Further details of the Bayesian implementation of the DFA model and extensions are provided in @ward2019.

### Modeling trends as Gaussian processes

Conventional DFA models with trends modeled as random walks are flexible, but for some datasets these models may be too complex. As a first alternative approach to the random walk model, we treat the trends as a Gaussian process. A discrete-time Gaussian process model of trends treats the vector representing the $k^{\mathrm{th}}$ trend as a stochastic process, where $\textbf{x}_{k}$ is drawn from a multivariate normal distribution. As data in a DFA are generally standardized (mean 0, standard deviation 0), we can assume the mean of each trend to be 0, and all inference about the Gaussian process centers around the covariance matrix, $\textbf{x}_{k} \sim \mathrm{MVN}(\textbf{0},\boldsymbol{\Sigma})$. Rather than estimate each element of $\boldsymbol{\Sigma}$ independently, smooth covariance functions or 'kernels' are chosen to represent the covariance between points in time (typical choices include the exponential, Gaussian, and MatÃ©rn functions). For the purpose of our DFA modeling, we adopt a Gaussian kernel. With this kernel, the covariance between points $i$ and $j$ at times ${t}_{i}$ and ${t}_{j}$ on trend $k$ can be expressed as $\mathrm{cov}({x}_{i,k},{x}_{j,k})={\sigma}_{k}^{2}\exp\left(\frac {-{\left({t}_{i}-{t}_{j} \right)}^{2}}{2{\theta}_{k}^{2}} \right)$, where ${\sigma}_{k}$ controls the magnitude of variation, and ${\theta}_{k}$ controls how smoothly correlation decreases as time points become further apart. We allow each trend to have its own covariance parameters, allowing each to have differing degrees of smoothness. Because of potential computation issues in high dimensionality problems such as spatial models [@latimer2009b; @anderson2019b], we also allow this Gaussian process model to be expressed as a Gaussian predictive process model. The difference between the predictive process approach and the full Gaussian process model is that instead of modeling the $\textbf{x}_{t}$ themselves as random variables, random variables are modeled at a subset of locations $\textbf{x}_{k}^{*}$ (these locations are referred to as 'knots') and projected to the locations of the data $\textbf{x}_{k}$. If we assume $\textbf{x}_{k}^{*} \sim \mathrm{MVN}(\textbf{0},{\boldsymbol{\Sigma}}^{*})$, then this projection can be done as ${{{x}_{k}=\boldsymbol{\Sigma'}}_{k,{k}^{*}}\boldsymbol{\Sigma}}^{*-1}{x}_{k}^{*}$, where the matrix $\boldsymbol{\Sigma'}_{k,{k}^{*}}$ is the transpose of the matrix describing the covariance between ${x}_{k}$ and ${x}_{k}^{*}$. The location of ${k}^{*}$ can be spaced equally or depend on data; for the purposes of our DFA modeling, we assume that the ${k}^{*}$ are equally spaced within each time series (with the endpoints also acting as knots). 

### Modeling trends as splines

As an alternative model of latent trends in a DFA, we use a series of smoothing functions, known as basis splines ('B-splines'). These models can be thought of as a special case of Gaussian process models [@kimeldorf1970a], and offer flexibility similar to the more familiar generalized additive models [@wood2011]. B-splines are represented as a series of piecewise polynomial functions, where higher order polynomials result in more flexible curves [@hastie1992]. A common choice of the order of these polynomials is a cubic or 3rd degree, and will be the focus of our implementation for DFA models. An additional input to B-splines is the locations of the control points between polynomial segments---more control points translate into a more flexible function, but also one with more parameters to estimate. Similar to Gaussian process models, the locations of these control points are referred to as knots, and are generally selected within the temporal domain of the data. For our applications to DFA, we assume knots to be uniformly distributed over the time series. Uniform knot vectors may be appropriate for data collected at regular intervals, but for observations more patchily distributed in time, distributing the knots based on quantiles or other metrics may be warranted. Mathematically, modeling the trends in a DFA with B-splines can be expressed as a linear combination of the B-spline weights $\textbf{B}$ and estimated coefficients $\textbf{a}$, $\textbf{x}_{k}=\textbf{a}\textbf{B}$. The matrix $\textbf{B}$ is generated from the raw data prior to estimation. In the DFA setting, $\textbf{B}$ is shared across trends, but for trend-specific variability, we allow the coefficients $\textbf{a}$ to have a variance parameter unique to each trend, $\textbf{a}_{k}\sim \mathrm{Normal}\left(0,{\sigma}^2_{k} \right)$.

### Application: 1-trend models of larval fish dynamics

As a first application of smooth factor analysis models, we apply DFA to a long term time series of larval fishes collected in Southern California (USA). The California Cooperative Oceanic Fisheries Investigations (CalCOFI) survey has been collecting physical and biological samples since 1949, to monitor annual, seasonal, and spatial changes to the California Current Ecosystem [@bograd2003]. The CalCOFI data have been incorporated into models used to assess population status [@maccall2003], and numerous publications have used time series of larval fishes from the CalCOFI survey as indicators of ecosystem state [@mcclatchie2008]. These types of motivating questions also present an opportunity to apply DFA with both conventional and smoothed trends to summarize ecosystem state indices. For this application, we focus on the dynamics of three species of juvenile rockfishes: Aurora rockfish (*Sebastes aurora*), Shortbelly rockfish (*S. jordani*), and Bocaccio rockfish (*S. paucispinis*). For the purposes of this analysis, we restrict the time series to data collected since 1985, when sampling has been consistent in space and time [@moser2001]. Though CalCOFI cruises are done throughout the year, we are primarily interested in estimating interannual trends, and thus restrict our analysis to considering spring cruises from 1-April to 22-May when densities of most rockfish species are highest [@mosek2000]. All data were retrieved using the software R [@rcoreteam2020] and the 'rerddap' package [@chamberlain2020].

With only three time series, we focus on identifying models estimating a single shared trend and single observation error variance, shared across species. Other types of models, including hierarchical GAMs [@pedersen2019] or models allowing estimated offsets may also be useful in this type of application. Where the DFA model differs is that unlike models with random intercepts or additive offset terms, the DFA factor loadings $\textbf{Z}$ are multiplicative and may be close to zero. These cases may arise when a particular time series has a low signal to noise ratio, or if there is low correspondence with the latent trends estimated among all other time series. In addition to estimating a conventional 1-trend DFA model with a latent autoregressive process, we evaluate 1-trend B-spline and Gaussian process models. Because we have no a priori hypotheses about the complexity of these smoothed factor models, we evaluated 5 models for each, using 6, 12, 18, 24, and 30 equally spaced knots.

### Application: 2-trend models of commercial fisheries catches  

As a slightly more complex example of the smooth factor analysis model, we examine the performance of 2-trend models, using a dataset of commercial fisheries catches (landings) from the west coast of the USA. This dataset consists of landings by dominant species, and is reported annually to the Pacific Fishery Management Council [@pfmc2020]. This dataset consists of 13 species or groups reported over a 39 year period (1981--2019). Landings on the west coast are dominated by Pacific hake (also Pacific whiting, *Merluccius productus*), but also include substantial catches of rockfishes (*Sebastes spp.*) and flatfishes (e.g., Dover sole, *Solea solea*). Over the course of the last 4 decades, these species have experienced variability associated with population dynamics and the environment, but the patterns of landings also reflects a dynamic fisheries management process. Examples of changes include temporarily closing areas to fishing to protect species of conservation concern, and implementing catch share programs. These processes, combined with environmental conditions that have been positive for many species have resulted in many increasing populations [@warlick2018]. Given these various management and ecological changes, it is important to summarize patterns of landings, and identify common trends as indicators for management and ecosystem status [@harvey2018b].

As with our previous example, we compared conventional DFA models to those modeling the trends with smooth functions. Preliminary model fitting suggested that 2-trend models were more supported by the data, and thus will be the focus of our analysis. In addition to modeling the 2-trend model with conventional DFA, we evaluated B-spline and Gaussian process models with 6 to 30 equally spaced knots. All models included a single observation error variance, shared across time series.

### Estimation and model selection  

We developed our DFA model in a Bayesian framework, using Stan and the package R package 'rstan' [@standevelopmentteam2016a], which implements Markov chain Monte Carlo (MCMC) using the No-U Turn Sampling (NUTS) algorithm [@hoffman2014; @carpenter2017a]. For each model considered, we ran 3 parallel MCMC chains for 3000 iterations each, discarding the first 50% of the samples. We assessed convergence using split-$\hat{R}$ and effective samples size [@gelman2013BDA] along with trace plots. Previous approaches have used the Leave One Out Information Criterion [LOOIC, @vehtari2017; @vehtari2020] as a model selection tool [@ward2019]. Preliminary model checks using LOOIC for the models included in our analysis indicated that many models had 1--4 data points that had high Pareto-k statistics [possibly because of model-misspecification or model flexibity, @vehtari2017]. As a slower but potentially more robust model selection approach, we implemented k-fold cross validation. There are many possible ways to assign 'folds' in cross-validation, and because of our focus on the temporal aspect of these DFA models, we assigned each year of data to a unique fold. Implementing these models using cross validation results in increased computational time---instead of fitting each model to a dataset once, a single model is re-fit to a dataset for as many years as there are data (35 years for our application to CalCOFI data; 39 years for our application to commercial landings data). 

As a measure of predictive accuracy, we used the cross-validation results for each of the models in our applications to calculate the expected log pointwise predictive density (ELPD). In the context of our cross-validation with each time step as a fold, the formula for ELPD is $ELPD = \sum _{k=1}^{T}{\log\left(p\left(\textbf{y}_{k}|\textbf{y}_{-k} \right)  \right)}$, where *k* indexes time steps $1,2,\ldots T$ and $p\left(\textbf{y}_{k}|\textbf{y}_{-k} \right) =\int {p\left(\textbf{y}_{k}|\theta  \right) p\left(\theta |\textbf{y}_{-k} \right) d\theta}$ [@vehtari2017]. Code for many classes of Bayesian models is available in the 'loo' package [@vehtari2020], and extensions for DFA models are made available in our 'bayesdfa' package [@ward2019].

## Results

```{r echo=FALSE}
m = readRDS("output/calcofi_models.rds")
fit = m[[2]]
r = bayesdfa::rotate_trends(fit)
post_means = apply(r$Z_rot,2,mean)
post_low = apply(r$Z_rot,2,quantile,0.025)
post_hi = apply(r$Z_rot,2,quantile,0.975)
# 
# # compare 6-knot GP and BS model
# fit_bs = m[[2]]
# fit_gp = m[[7]]
# x_bs = apply(rstan::extract(fit_bs$model,"x")$x,3,mean)
# x_gp = apply(rstan::extract(fit_gp$model,"x")$x,3,mean)
```

For our application of smooth dynamic factor models to the CalCOFI juvenile rockfish dataset, we found that the B-spline trend model with 6 knots had higher ELPD values compared to alternative models (Table 1). Varying the number of knots for the B-spline or Gaussian process models had little effect on predictive accuracy, and the B-spline models performed slighlty better than the Gaussian process models (Table 1). Varying the number of knots did allow for greater flexibility, however, allowing for more complex models to better capture recent variability in rockfish densities. All smooth trend models had higher predictive accuracy compared to the conventional random walk DFA model. Estimated loadings for each of the three *Sebastes* species were positive, with shortbelly rockfishes having the highest loading (`r round(post_means[3],2)`, 95% posterior interval = `r round(post_low[3],2)`--`r round(post_hi[3],2)`) followed by aurora (`r round(post_means[1],2)`, 95% posterior interval = `r round(post_low[1],2)`--`r round(post_hi[1],2)`) and bocaccio rockfishes with the weakest loadings (`r round(post_means[2],2)`, 95% posterior interval = `r round(post_low[2],2)`--`r round(post_hi[2],2)`). Although the model with 6 knots was found to have the highest predictive accuracy, the simplicity of this model was not able to capture the most recent patterns between 2015--2019 when juvenile rockfish experienced high densities and variability (Fig.~1).

When smooth-trend B-spline and Gaussian predictive process models were applied to commercial fisheries landings data, the model with the highest ELPD was the B-spline model with 24 knots. Across all models, those that modeled trends as B-splines had higher predictive accuracy than those that modeled trends as Gaussian processes or random walks (Table 2). All models considered for this dataset included 2 latent trends. The first trend exhibited nearly linear change from 1981--2001 and was relatively stationary from 2001--2019 (Fig.~2). The second trend represented change from the early 1990s, with the strongest change occuring 2010--present. Estimates of the loadings from the model with highest ELPD indicated most species or species groups loaded negatively on trend 1, with the exception of Pacific whiting (Fig. 3). Trend 2 from this model appeared to contrast species with relatively stationary catches before declining in 2010 (e.g., Arrowtooth flounder, *Atheresthes stomas*) versus Petrale sole (*Eopsetta jordani*)---one of the only non-whiting species that has experienced positive catches since 2010. Predictions across all models appeared to characterize the trends of most species, although none of the approaches were able to capture the variability in Pacific whiting catches since 2000 (Fig.~4).

## Discussion

Dynamic factor analysis (DFA) represents a flexible approach for using state space models to capture latent processes in multivariate time series [@zuur2003b; @zuur2003c]. For some ecological datasets, using a random walk to model the latent DFA trends may be overly complex. Examples of cases where random walks may overfit trends may be when there are large gaps in time between observations, or data collected from systems with high signal to noise ratios. 
<!-- TODO: SA: reverse? -->
As alternatives to the conventional random walk, we illustrate how DFA trends may be modeled using Gaussian process models or B-splines. Both of these alternatives are flexible in that their smoothness may be specified a priori by the user, and compared via model selection. As the variability of latent trends is nearly always fixed in a conventional DFA for identifiability [@zuur2003b; @holmes2012b], adopting an alternative model of the trend does not limit inference or change the meaning of other parameters (e.g., loadings). 

In both of our case studies comparing smooth DFA models to conventional ones, we found that using smooth functions to model DFA trends resulted in models with higher predictive ability (compared with out of sample cross-validation). Models that used B-splines to model trends received the highest support in both applications, and that these models had better predictive ability than Gaussian predictive process models. In our evaluation of 1-trend DFA models to the CalCOFI juvenile rockfish dataset, varying the number of knots ($> 6$) had little effect on predictive performance. While these trends are relatively stationary and may be explained by simpler models, the insensitivity of predictive accuracy to more complex models may also be driven by the inability of all models to predict extreme deviations (Figure 1).

We used uniform knots for both types of smooth trend models, and these results would be expected to change slightly if the knot locations were adjusted. For models with missing data, or datasets with unevenly distributed replicate samples, it may be important to consider non-uniform knot vectors. 

Paragraph here on similarities and differences in the models and interpretation
A couple sentences about comparing GP / BS models, flexibility, etc. 
- B-spline models result in predictions in any year that generally has slightly higher ELPD

Because of their flexibility, applications of LOOIC or related model selection to DFA models may result in poor diagnostics. As an alternative, we implemented cross-validation in two applications to identify the model with the highest predictive accuracy. In each case, we implemented k-fold cross validation with each year of the time series held out in turn and predicted from the remaining data. Our first application of these methods to the CalCOFI juvenile rockfish dataset highlighted that models with high predictive accuracy across the full time series may not be the best for capturing variation in particular periods of interest. Between 2015 - 2019 for example, the three rockfish species in our case study experienced a pulse that was captured when DFA trends were modeled as a random walk, but not by smooth trend models (Fig. 1) - despite those models having higher predictive accuracy across the time series. Several alternative approaches to cross validation may be useful if predictions in some years are more important than others. For example, the analysis of CalCOFI data could be repeated, but the ELPD could be calculated using only the last decade as test data. A second approach would be to implement leave-future-out cross-validation, which differs from our approach in that data points are only used to predict future years, not historical values [@burkner2020a]. While none of the models included in our analysis included covariates, the cross-validation approaches described here can be extended identify predictor variables (such as environmental covariates) that result in increased predictive accuracy.

## Acknowledgments

\break

## Tables 

Table 1. Expected log posterior density (ELPD, with standard errors in parentheses) for each of the models applied to our cases studies (CalCOFI time series of juvenile rockfishes, and the time series of commercial groundfish landings from the west coast of the USA). The B-spline models are generated with basis splines, and the Gaussian predictive process models are generated using a Gaussian covariance function. For each model, knots (or locations of control points) are assumed to be uniformly spaced over the time series. To aid in interpretation, the minimum ELPD value across models has been subtracted from each case study.

```{r table1, echo=FALSE}
# more stable than log(sum(exp(x)))
log_sum_exp <- function(x) {
  max_x <- max(x)
  max_x + log(sum(exp(x - max_x)))
}

# more stable than log(mean(exp(x)))
log_mean_exp <- function(x) {
  log_sum_exp(x) - log(length(x))
}

# this loop -- and one below -- are just to drop first point out
# of calculation. this is just the way the bayesdfa code is setup -- 
# x0 is a parameter at t = 1, but this wouldn't be necessary if t = 0
cv_calcofi = readRDS("output/calcofi_models_cv.rds")
for(i in 1:length(cv_calcofi)) {
  elpds <- apply(cv_calcofi[[i]]$log_lik,2,log_sum_exp)
  cv_calcofi[[i]]$elpd_kfold = sum(elpds[-1])
  cv_calcofi[[i]]$se_elpd_kfold = sqrt(length(elpds[-1]) * var(elpds[-1]))
}
min_calcofi = min(round(unlist(lapply(cv_calcofi,getElement,3)), 2))

cv_landings = readRDS("output/landings_models_cv.rds")
for(i in 1:length(cv_landings)) {
  elpds <- apply(cv_landings[[i]]$log_lik,2,log_sum_exp)
  cv_landings[[i]]$elpd_kfold = sum(elpds[-1])
  cv_landings[[i]]$se_elpd_kfold = sqrt(length(elpds[-1]) * var(elpds[-1]))
}
min_landings = min(round(unlist(lapply(cv_landings,getElement,3)), 2))

tab = data.frame("Trend model"=c("Random walk", rep("B-spline",5), rep("Gaussian process",5)),
  "Knots" = c(NA, rep(seq(6,30,by=6),2)),
  "CalCOFI" = paste0(round(unlist(lapply(cv_calcofi,getElement,3)) - min_calcofi, 2), " (",round(unlist(lapply(cv_calcofi,getElement,4)),2),")"),
  "Landings" = paste0(round(unlist(lapply(cv_landings,getElement,3)) - min_landings, 2), " (",round(unlist(lapply(cv_landings,getElement,4)),2),")")
  )
knitr::kable(tab)
```

\break

## Figure Captions 
Figure 1. Standardized densities of juvenile shortbelly rockfish collected in the CalCOFI survey, and estimates of latent trends for three candidate models, representing a range of flexibility in splines compared to the conventional random walk. A B-spline model with 6 knots had slighty higher ELPD values than other smooth trend models (such as the more coarse 12 knot model) and the conventional DFA (random walk). The posterior mean from each model is shown as a solid black line, and 95% credible intervals are shown in the grey region. 

Figure 2. Estimated trends from the 2-trend DFA model applied to commercial groundfish landings off the west coast of the United States. The model results with highest ELPD is shown, a model that allows trends to be approximated with B-spines (24 knots). The posterior mean for each trend is shown, with ribbons representing 95% credible intervals. 

Figure 3. Estimated loadings for each species or group from a 2-trend DFA model with latent trends modeled as B-splines. The posterior mean for each species is shown as a point, with lines representing 95% credible intervals. 

Figure 4. Estimated landings for 2 species included in our analysis, with contrasting trends (lingcod, Pacific whiting). Posterior means and 95% credible intervals (ribbons) for three candidate models are shown: b-spline trend models with 6 and 18 knots, respectively, and a random walk model representing the conventional DFA model.

\break

```{r fig1, echo=FALSE, fig.pos="placeHere", fig.cap="Figure 01 \\label{fig:fig1}", fig.height=6}
m = readRDS("output/calcofi_models.rds")

# standardize raw data
x = readRDS("output/calcofi_data.rds")
scaled_x = group_by(x, ts) %>%
  dplyr::mutate(obs = (obs-mean(obs,na.rm=T))/sd(obs,na.rm=T))
scaled_x$Species = c("aurora","shortbelly","bocaccio")[scaled_x$ts]
scaled_x = dplyr::filter(scaled_x, Species=="shortbelly")

pred = predicted(m[[2]])[,,,3]
# n_mcmc / n_chains / n_years / n_ts
trend_df1 = data.frame("time"= seq(min(x$time),max(x$time)),
  "low" = apply(pred,3,quantile,0.025),
  "mean"=apply(pred,3,mean),
  "hi"=apply(pred,3,quantile,0.975),
  "Model" = "B-spline, 6 knots")

pred = predicted(m[[4]])[,,,3]
r = bayesdfa::rotate_trends(fit)
trend_df2 = data.frame("time"= seq(min(x$time),max(x$time)),
  "low" = apply(pred,3,quantile,0.025),
  "mean"=apply(pred,3,mean),
  "hi"=apply(pred,3,quantile,0.975),
  "Model" = "B-spline, 18 knots")

pred = predicted(m[[1]])[,,,3]
r = bayesdfa::rotate_trends(fit)
trend_df3 = data.frame("time"= seq(min(x$time),max(x$time)),
  "low" = apply(pred,3,quantile,0.025),
  "mean"=apply(pred,3,mean),
  "hi"=apply(pred,3,quantile,0.975),
  "Model" = "Random walk")

trend_df = rbind(trend_df1, trend_df2, trend_df3)

g1 = ggplot(trend_df, aes(time,mean)) +
  geom_ribbon(aes(ymin=low,ymax=hi),alpha=0.2) +
  geom_line() +
  theme_bw() +
  facet_wrap(~Model, nrow=3, scale="free_y") + 
  theme(strip.background =element_rect(fill="white")) + 
  geom_point(data=scaled_x, aes(x=time,y=obs,col=Species),alpha=0.7,size=3) +
  scale_color_manual("Species",
    values = c("shortbelly" = "#2A788EFF")) +
  #scale_color_manual("Species",
  #  values = c("aurora" = "#440154FF", "shortbelly" = "#2A788EFF", "bocaccio" = "#7AD151FF")) +
  ylab("Standardized densities and trend") +
  xlab("Year") + 
  theme(legend.position = "none")

print(g1)
```

\break  
  
```{r fig2, echo=FALSE, fig.pos="placeHere", fig.cap="Figure 02 \\label{fig:fig4}"}

# make plots for best model
m = readRDS("output/landings_bs_24_knots.rds")
r = rotate_trends(m)

g1 = plot_trends(r, years = 1981:2019) + 
  theme_bw() + theme(strip.background=element_rect(fill="white"))
print(g1)
```

\break  

```{r fig3, echo=FALSE, fig.pos="placeHere", fig.cap="Figure 03 \\label{fig:fig3}"}
# bring in raw data
d = read.csv("data/port_landings_table2.csv", stringsAsFactors = FALSE)
d = dplyr::select(d, -Year)
for(i in 1:ncol(d)) {
  d[,i] = log(as.numeric(d[,i]))
}

# bring in best model
# make plots for best model
m = readRDS("output/landings_bs_24_knots.rds")
r = rotate_trends(m)

loadings = as.data.frame(cbind(apply(r$Z_rot,c(2,3),mean), apply(r$Z_rot,c(2,3),sd)))
names(loadings) = c("Trend1_mean","Trend2_mean","Trend1_sd","Trend2_sd")
names(d)[1] = "Pacific whiting"
names(d)[4] = "Pacific cod"
names(d)[5] = "Misc roundfish"
names(d)[8] = "Arrowtooth flounder"
names(d)[9:13] = c("Dover sole","English sole","Petrale sole","Misc flatfish","Misc groundfish")
loadings$species = names(d)

g1 = ggplot(dplyr::filter(loadings, species!="Misc roundfish",species!="Misc flatfish",species!="Misc groundfish"),
  aes(Trend1_mean,Trend2_mean,label=species)) +
  geom_point(col="darkblue") +
  geom_errorbar(aes(ymin=Trend2_mean - Trend2_sd,ymax=Trend2_mean + Trend2_sd),col="darkblue",alpha=0.3) +
  geom_errorbarh(aes(xmin=Trend1_mean - Trend1_sd,xmax=Trend1_mean + Trend1_sd),col="darkblue",alpha=0.3) +
  geom_text_repel(size=5,col="dark blue") +
  theme_bw() +
  xlab("Loading on trend 1") +
  ylab("Loading on trend 2")

print(g1)
```

\break  

```{r fig4, echo=FALSE, fig.pos="placeHere", fig.cap="Figure 04 \\label{fig:fig4}"}

df = readRDS("output/predicted_lingcod_whiting.rds")

# bring in raw observations
d = read.csv("data/port_landings_table2.csv", stringsAsFactors = FALSE)
obs = data.frame(year = rep(1981:2019,2),
  y = c(d[["Lingcod"]],d[["P..Whiting"]]),
  "Species"=c(rep("Lingcod",39), rep("Pacific whiting",39)))
obs = dplyr::group_by(obs, Species) %>%
  dplyr::mutate(y = (y-mean(y))/sd(y),
    Model="Random walk")

g1 = ggplot(df, aes(year, pred, group=Model, col=Model,fill=Model)) +
  geom_ribbon(aes(ymin=lower,ymax=upper),alpha=0.3,col=NA) +
  geom_line() +
  theme_bw() +
  xlab("") +
  ylab("Standardized landings (mt)") +
  scale_fill_viridis(end=0.8, discrete = TRUE) +
  scale_color_viridis(end=0.8, discrete = TRUE) +
  facet_wrap(~ Species) +
  theme(strip.background =element_rect(fill="white")) +
  geom_point(data=obs, aes(year, y),size=2, col="grey30",alpha=0.5)

print(g1)
```

\break
  
  
# References
